ğŸ“˜ PDF Q&A Chatbot

A Streamlit-based application that allows users to upload a PDF and ask questions related to its content. The system extracts text and tables from the PDF, creates semantic embeddings, performs vector search using FAISS, and generates answers using an LLM hosted on Hugging Face Inference API.

ğŸš€ Features

ğŸ“„ PDF Upload Support
Users can upload any PDF and extract text + tables.

ğŸ” Semantic Search with FAISS
Relevant chunks are retrieved using vector similarity.

ğŸ¤– LLM-Powered Answers
Uses the Llama 3.2 3B Instruct model (free on Hugging Face) via HuggingFaceEndpoint.

ğŸ§  Embeddings with Sentence Transformers
Uses all-MiniLM-L6-v2 for lightweight, accurate vector embeddings.

ğŸ§¾ Transparent Debug Info (Optional)
Shows retrieved text chunks and context preview for debugging.

ğŸ§± Streamlit UI
Simple, responsive interface for interactive question answering.

ğŸ“‚ Project Structure
ğŸ“ project-root/
â”‚â”€â”€ app.py                # Main Streamlit application
â”‚â”€â”€ requirements.txt      # Python dependencies
â”‚â”€â”€ README.md             # Project documentation
â”‚â”€â”€ .env                  # Environment variables (API keys)

ğŸ› ï¸ Technologies Used
Component	Technology
UI	Streamlit
PDF Parsing	PyMuPDF (fitz), pdfplumber
Embeddings	sentence-transformers
Vector DB	FAISS
LLM	HuggingFaceEndpoint (Llama 3.2 3B Instruct)
Environment	Python 3.11, virtualenv/pyenv
âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the repository
git clone https://github.com/priyamvadasingh2511/pdf-qa-chatbot.git
cd pdf-qa-chatbot

2ï¸âƒ£ Create & activate virtual environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate     # Mac / Linux
venv\Scripts\activate        # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add your Hugging Face API Token

Create a .env file:

HUGGINGFACEHUB_API_TOKEN=your_token_here


Get your token here:
https://huggingface.co/settings/tokens

5ï¸âƒ£ Run the application
streamlit run app.py


Open browser at:
http://localhost:8501

ğŸ§  How It Works (Architecture)

User uploads PDF
â†’ App extracts raw text using PyMuPDF + tables using pdfplumber.

Text is chunked and embedded
â†’ Embeddings generated using all-MiniLM-L6-v2.

FAISS Index is built
â†’ Enables fast similarity search.

User asks a question
â†’ Embedding of question is generated, FAISS returns top-K chunks.

LLM receives prompt containing

Retrieved context

User question

LLM generates answer using HuggingFace model.

ğŸ“ Example Prompt Sent to LLM
You are an expert AI assistant. Answer strictly based on the provided context. 
If the context does not contain the answer, reply 'I don't know'.

Context:
<retrieved PDF text>

Question:
<user question>

Answer:

ğŸ“Š Debug Information

The app shows:

Relevant text chunks

Context preview sent to the LLM

Embedding shapes

Sample embedding vector

This helps validate whether:

The PDF was parsed correctly

FAISS indexing works

Retrieval is accurate

ğŸ”’ Environment Variables
Variable	Description
HUGGINGFACEHUB_API_TOKEN	Token to access Hugging Face Inference API
ğŸš§ Future Enhancements

ğŸ” Highlight PDF text used to generate answer

ğŸ—‚ï¸ Use better chunking (overlapping windows)

ğŸ“„ Support multiple PDFs at once

ğŸ¤– Option to switch between LLMs

ğŸ§© Add streaming responses

ğŸ¤ Contributing

Contributions are welcome!
Please open an issue or submit a pull request.
